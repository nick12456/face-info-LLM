<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Face Info + LLM</title>
<style>
  body{margin:0;background:#000;color:#fff;font-family:system-ui,-apple-system,Segoe UI,Roboto,Inter}
  video,canvas{position:fixed;inset:0;width:100%;height:100%;object-fit:cover}
  #status{position:fixed;top:10px;left:10px;background:#0008;padding:6px 10px;border-radius:8px}
  #llmResponse{position:fixed;bottom:10px;left:10px;right:10px;background:#000c;padding:10px;border-radius:8px;min-height:30px}
</style>
</head>
<body>
  <video id="video" autoplay playsinline muted></video>
  <canvas id="canvas"></canvas>
  <div id="status">loadingâ€¦</div>
  <div id="llmResponse">ðŸ¤– Waitingâ€¦</div>

  <!-- Face API -->
  <script src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api/dist/face-api.min.js"></script>
  <script>
  const MODEL_URL="https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model/";
  const video=document.getElementById("video");
  const statusBox=document.getElementById("status");
  const llmBox=document.getElementById("llmResponse");

  async function startCamera(){
    try{
      const stream=await navigator.mediaDevices.getUserMedia({video:{facingMode:"user", width:{ideal:640}, height:{ideal:480}}});
      video.srcObject=stream; await video.play();
    }catch(e){
      statusBox.innerText = "Camera error: " + e.message + " (Open over HTTPS and allow camera)";
    }
  }

  async function loadModels(){
    await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
    await faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL);
    await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
    statusBox.innerText="âœ… Models loaded";
  }

  // Calls your serverless backend (/api/llm) that talks to OpenAI with a secret key
  async function askLLM(data){
    try{
      const res=await fetch("/api/llm",{
        method:"POST",
        headers:{"Content-Type":"application/json"},
        body:JSON.stringify(data)
      });
      const json=await res.json();
      if(json.reply){ llmBox.innerText="ðŸ¤– "+json.reply; }
      else { llmBox.innerText="âš ï¸ LLM error: " + (json.error||"unknown"); }
    }catch(e){ llmBox.innerText="âš ï¸ LLM network error: "+e.message; }
  }

  async function runLoop(){
    const opts=new faceapi.TinyFaceDetectorOptions({inputSize:160, scoreThreshold:0.5});
    setInterval(async()=>{
      try{
        const res=await faceapi.detectSingleFace(video,opts)
          .withAgeAndGender().withFaceExpressions();
        if(res){
          const age=Math.round(res.age);
          const gender=res.gender;
          const exprs=res.expressions||{};
          let topE="",topV=0;
          for(const k in exprs){ if(exprs[k]>topV){ topE=k; topV=exprs[k]; } }
          statusBox.innerText=`age:${age}, gender:${gender}, mood:${topE}`;
          askLLM({age,gender,emotion:topE});
        }
      }catch(e){ /* ignore occasional frame errors */ }
    },3000);
  }

  (async()=>{
    await startCamera();
    await loadModels();
    runLoop();
  })();
  </script>
</body>
</html>
